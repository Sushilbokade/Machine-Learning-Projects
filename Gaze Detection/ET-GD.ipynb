{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# from PIL import ImageGrab\n",
    "# import dlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EyeTracker:\n",
    "#     def __init__(self, model_file=\"shape_predictor_68_face_landmarks.dat\"):\n",
    "#         # Load the facial landmark detector.\n",
    "#         self.detector = dlib.get_frontal_face_detector()\n",
    "#         self.predictor = dlib.shape_predictor(model_file)\n",
    "\n",
    "#     def get_eye_region(self, landmarks, eye_indices):\n",
    "#         # Extract the (x, y)-coordinates of the landmarks for the eye region.\n",
    "#         points = np.array([(landmarks.part(index).x, landmarks.part(index).y) for index in eye_indices])\n",
    "\n",
    "#         # Compute the bounding box for the eye region.\n",
    "#         x, y, w, h = cv2.boundingRect(points)\n",
    "\n",
    "#         # Extract the eye region from the grayscale image.\n",
    "#         eye_region = gray[y:y + h, x:x + w]\n",
    "\n",
    "#         # Return the eye region.\n",
    "#         return eye_region\n",
    "\n",
    "#     def get_gaze_point(self, eye_region, face, screen_width, screen_height):\n",
    "#         # Compute the center of the eye region.\n",
    "#         eye_center = (face.left() + eye_region.shape[1] / 2, face.top() + eye_region.shape[0] / 2)\n",
    "\n",
    "#         # Apply a threshold to the eye region to create a binary image.\n",
    "#         threshold = cv2.threshold(eye_region, 30, 255, cv2.THRESH_BINARY)[1]\n",
    "\n",
    "#         # Find the contours in the binary image.\n",
    "#         contours, _ = cv2.findContours(threshold, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "#         # Find the contour with the largest area.\n",
    "#         if len(contours) > 0:\n",
    "#             contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "#             # Compute the center of the contour.\n",
    "#             moments = cv2.moments(contour)\n",
    "#             if moments[\"m00\"] > 0:\n",
    "#                 cx = int(moments[\"m10\"] / moments[\"m00\"])\n",
    "#                 cy = int(moments[\"m01\"] / moments[\"m00\"])\n",
    "#                 gaze_point = (int(screen_width * cx / eye_region.shape[1]), int(screen_height * cy / eye_region.shape[0]))\n",
    "#                 return gaze_point\n",
    "\n",
    "#         # If no gaze point is found, return the center of the eye region.\n",
    "#         return (int(eye_center[0] * screen_width / gray.shape[1]), int(eye_center[1] * screen_height / gray.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dlib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 58\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[39mreturn\u001b[39;00m left_eye, right_eye\n\u001b[0;32m     57\u001b[0m \u001b[39m# Initialize the eye tracker.\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m tracker \u001b[39m=\u001b[39m EyeTracker()\n",
      "Cell \u001b[1;32mIn [1], line 4\u001b[0m, in \u001b[0;36mEyeTracker.__init__\u001b[1;34m(self, model_file)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, model_file\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshape_predictor_68_face_landmarks.dat\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m      3\u001b[0m     \u001b[39m# Load the facial landmark detector.\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdetector \u001b[39m=\u001b[39m dlib\u001b[39m.\u001b[39mget_frontal_face_detector()\n\u001b[0;32m      5\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor \u001b[39m=\u001b[39m dlib\u001b[39m.\u001b[39mshape_predictor(model_file)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dlib' is not defined"
     ]
    }
   ],
   "source": [
    "# class EyeTracker:\n",
    "#     def __init__(self, model_file=\"shape_predictor_68_face_landmarks.dat\"):\n",
    "#         # Load the facial landmark detector.\n",
    "#         self.detector = dlib.get_frontal_face_detector()\n",
    "#         self.predictor = dlib.shape_predictor(model_file)\n",
    "\n",
    "#     def get_eye_region(self, landmarks, eye_indices):\n",
    "#         # Extract the (x, y)-coordinates of the landmarks for the eye region.\n",
    "#         points = np.array([(landmarks.part(index).x, landmarks.part(index).y) for index in eye_indices])\n",
    "\n",
    "#         # Compute the bounding box for the eye region.\n",
    "#         x, y, w, h = cv2.boundingRect(points)\n",
    "\n",
    "#         # Extract the eye region from the grayscale image.\n",
    "#         eye_region = gray[y:y + h, x:x + w]\n",
    "\n",
    "#         # Return the eye region.\n",
    "#         return eye_region\n",
    "\n",
    "#     def get_gaze_point(self, eye_region, face, screen_width, screen_height):\n",
    "#         # Compute the center of the eye region.\n",
    "#         eye_center = (face.left() + eye_region.shape[1] / 2, face.top() + eye_region.shape[0] / 2)\n",
    "\n",
    "#         # Apply a threshold to the eye region to create a binary image.\n",
    "#         threshold = cv2.threshold(eye_region, 30, 255, cv2.THRESH_BINARY)[1]\n",
    "\n",
    "#         # Find the contours in the binary image.\n",
    "#         contours, _ = cv2.findContours(threshold, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "#         # Find the contour with the largest area.\n",
    "#         if len(contours) > 0:\n",
    "#             contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "#             # Compute the center of the contour.\n",
    "#             moments = cv2.moments(contour)\n",
    "#             if moments[\"m00\"] > 0:\n",
    "#                 cx = int(moments[\"m10\"] / moments[\"m00\"])\n",
    "#                 cy = int(moments[\"m01\"] / moments[\"m00\"])\n",
    "#                 gaze_point = (int(screen_width * cx / eye_region.shape[1]), int(screen_height * cy / eye_region.shape[0]))\n",
    "#                 return gaze_point\n",
    "\n",
    "#         # If no gaze point is found, return the center of the eye region.\n",
    "#         return (int(eye_center[0] * screen_width / gray.shape[1]), int(eye_center[1] * screen_height / gray.shape[0]))\n",
    "\n",
    "# def extract_eye_regions(landmarks):\n",
    "#     # Get the indices of the landmarks for the left and right eyes.\n",
    "#     left_eye_indices = list(range(36, 42))\n",
    "#     right_eye_indices = list(range(42, 48))\n",
    "\n",
    "#     # Extract the left and right eye regions from the facial landmarks.\n",
    "#     left_eye = tracker.get_eye_region(landmarks, left_eye_indices)\n",
    "#     right_eye = tracker.get_eye_region(landmarks, right_eye_indices)\n",
    "\n",
    "#     # Return the left and right eye regions.\n",
    "#     return left_eye, right_eye\n",
    "\n",
    "# # Initialize the eye tracker.\n",
    "# tracker = EyeTracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [6], line 60\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[39m# Save the screenshot with the gaze points overlaid.\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m screenshot \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(screen)\n\u001b[0;32m     61\u001b[0m screenshot\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mscreenshot_gaze.png\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     63\u001b[0m \u001b[39m# Release the video capture device and close the window.\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Initialize the eye tracker.\n",
    "# tracker = EyeTracker()\n",
    "\n",
    "# # Capture the screen.\n",
    "# screen = np.array(ImageGrab.grab())\n",
    "\n",
    "# # Get the width and height of the screen.\n",
    "# screen_width, screen_height = screen.shape[1], screen.shape[0]\n",
    "\n",
    "# # Initialize the video capture device.\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# # Initialize the facial landmark detector.\n",
    "# detector = dlib.get_frontal_face_detector()\n",
    "# predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# # Initialize the window to display the video stream.\n",
    "# cv2.namedWindow(\"Gaze Tracking\")\n",
    "\n",
    "# # Loop through the frames from the video capture device.\n",
    "# while True:\n",
    "#     # Capture a frame from the video capture device.\n",
    "#     ret, frame = cap.read()\n",
    "\n",
    "#     # Convert the frame to grayscale.\n",
    "#     gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#     # Detect faces in the grayscale frame.\n",
    "#     faces = detector(gray)\n",
    "\n",
    "#     # Loop through the faces.\n",
    "#     for face in faces:\n",
    "#         # Get the facial landmarks for the face.\n",
    "#         landmarks = predictor(gray, face)\n",
    "\n",
    "#         # Extract the eye regions from the facial landmarks.\n",
    "#         left_eye, right_eye = extract_eye_regions(landmarks)\n",
    "\n",
    "#         # Compute the gaze points for the left and right eyes.\n",
    "#         left_gaze_point = tracker.get_gaze_point(left_eye, face, screen_width, screen_height)\n",
    "#         right_gaze_point = tracker.get_gaze_point(right_eye, face, screen_width, screen_height)\n",
    "\n",
    "#         # Draw circles on the frame to indicate the gaze points.\n",
    "#         cv2.circle(frame, left_gaze_point, 2, (0, 0, 255), -1)\n",
    "#         cv2.circle(frame, right_gaze_point, 2, (0, 0, 255), -1)\n",
    "\n",
    "#         # Draw circles on the screen to indicate the gaze points.\n",
    "#         cv2.circle(screen, left_gaze_point, 2, (0, 0, 255), -1)\n",
    "#         cv2.circle(screen, right_gaze_point, 2, (0, 0, 255), -1)\n",
    "\n",
    "#     # Display the video stream with the gaze points overlaid.\n",
    "#     cv2.imshow(\"Gaze Tracking\", frame)\n",
    "\n",
    "#     # Check for key press events.\n",
    "#     key = cv2.waitKey(1)\n",
    "#     if key == ord(\"q\"):\n",
    "#         break\n",
    "\n",
    "# # Save the screenshot with the gaze points overlaid.\n",
    "# screenshot = Image.fromarray(screen)\n",
    "# screenshot.save(\"screenshot_gaze.png\")\n",
    "\n",
    "# # Release the video capture device and close the window.\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (909160946.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [1], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    from gaze-tracking import GazeTracking\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from gaze-tracking import GazeTracking\n",
    "!pip install gaze-tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.6.0) :-1: error: (-5:Bad argument) in function 'boundingRect'\n> Overload resolution failed:\n>  - array data type = 17 is not supported\n>  - Expected Ptr<cv::UMat> for argument 'array'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32md:\\OneDrive - jainuniversity.ac.in\\D Drive\\Study\\PCL\\Final Report\\Gazedetect\\gaze-tracking-heatmap-main\\ET-GD.ipynb Cell 6\u001b[0m in \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20jainuniversity.ac.in/D%20Drive/Study/PCL/Final%20Report/Gazedetect/gaze-tracking-heatmap-main/ET-GD.ipynb#W5sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m right_eye \u001b[39m=\u001b[39m shape\u001b[39m.\u001b[39mpart(\u001b[39m45\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20jainuniversity.ac.in/D%20Drive/Study/PCL/Final%20Report/Gazedetect/gaze-tracking-heatmap-main/ET-GD.ipynb#W5sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39m# Extract the iris regions of the left and right eyes\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20jainuniversity.ac.in/D%20Drive/Study/PCL/Final%20Report/Gazedetect/gaze-tracking-heatmap-main/ET-GD.ipynb#W5sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m left_iris \u001b[39m=\u001b[39m extract_iris(gray, [left_eye])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20jainuniversity.ac.in/D%20Drive/Study/PCL/Final%20Report/Gazedetect/gaze-tracking-heatmap-main/ET-GD.ipynb#W5sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m right_iris \u001b[39m=\u001b[39m extract_iris(gray, [right_eye])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20jainuniversity.ac.in/D%20Drive/Study/PCL/Final%20Report/Gazedetect/gaze-tracking-heatmap-main/ET-GD.ipynb#W5sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39m# Compute the gaze point\u001b[39;00m\n",
      "\u001b[1;32md:\\OneDrive - jainuniversity.ac.in\\D Drive\\Study\\PCL\\Final Report\\Gazedetect\\gaze-tracking-heatmap-main\\ET-GD.ipynb Cell 6\u001b[0m in \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20jainuniversity.ac.in/D%20Drive/Study/PCL/Final%20Report/Gazedetect/gaze-tracking-heatmap-main/ET-GD.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_iris\u001b[39m(image, landmarks):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20jainuniversity.ac.in/D%20Drive/Study/PCL/Final%20Report/Gazedetect/gaze-tracking-heatmap-main/ET-GD.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m# Define the region of interest around the eye\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20jainuniversity.ac.in/D%20Drive/Study/PCL/Final%20Report/Gazedetect/gaze-tracking-heatmap-main/ET-GD.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     x, y, w, h \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mboundingRect(np\u001b[39m.\u001b[39;49marray(landmarks))\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20jainuniversity.ac.in/D%20Drive/Study/PCL/Final%20Report/Gazedetect/gaze-tracking-heatmap-main/ET-GD.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     eye_roi \u001b[39m=\u001b[39m image[y:y\u001b[39m+\u001b[39mh, x:x\u001b[39m+\u001b[39mw]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20jainuniversity.ac.in/D%20Drive/Study/PCL/Final%20Report/Gazedetect/gaze-tracking-heatmap-main/ET-GD.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m# Threshold the eye region to extract the iris\u001b[39;00m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.6.0) :-1: error: (-5:Bad argument) in function 'boundingRect'\n> Overload resolution failed:\n>  - array data type = 17 is not supported\n>  - Expected Ptr<cv::UMat> for argument 'array'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import imutils\n",
    "import numpy as np\n",
    "\n",
    "def extract_iris(image, landmarks):\n",
    "    # Define the region of interest around the eye\n",
    "    x, y, w, h = cv2.boundingRect(np.array(landmarks))\n",
    "    eye_roi = image[y:y+h, x:x+w]\n",
    "\n",
    "    # Threshold the eye region to extract the iris\n",
    "    _, iris = cv2.threshold(eye_roi, 50, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Smooth the iris region\n",
    "    iris = cv2.medianBlur(iris, 7)\n",
    "\n",
    "    # Resize the iris to a fixed size\n",
    "    iris = cv2.resize(iris, (32, 32))\n",
    "\n",
    "    return iris\n",
    "\n",
    "def track_gaze(left_iris, right_iris):\n",
    "    # Determine the center of each iris\n",
    "    left_center = np.unravel_index(np.argmax(left_iris), left_iris.shape)\n",
    "    right_center = np.unravel_index(np.argmax(right_iris), right_iris.shape)\n",
    "\n",
    "    # Compute the average of the iris centers to get the gaze point\n",
    "    gaze_x = (left_center[1] + right_center[1]) / 2\n",
    "    gaze_y = (left_center[0] + right_center[0]) / 2\n",
    "\n",
    "    return gaze_x, gaze_y\n",
    "\n",
    "# Load the face and eye detectors from Dlib\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"D:\\OneDrive - jainuniversity.ac.in\\D Drive\\Study\\PCL\\Final Report\\Gazedetect\\gaze-tracking-heatmap-main\\shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# Open a video capture stream\n",
    "screen = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Read the next frame from the video stream\n",
    "    ret, frame = screen.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Error capturing screen.\")\n",
    "        break\n",
    "\n",
    "    # Convert the frame to grayscale and resize it\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    gray = imutils.resize(gray, width=500)\n",
    "\n",
    "    # Detect faces in the image\n",
    "    rects = detector(gray, 0)\n",
    "\n",
    "    for rect in rects:\n",
    "        # Extract the landmarks of the left and right eyes\n",
    "        shape = predictor(gray, rect)\n",
    "        left_eye = shape.part(36)\n",
    "        right_eye = shape.part(45)\n",
    "\n",
    "        # Extract the iris regions of the left and right eyes\n",
    "        left_iris = extract_iris(gray, [left_eye])\n",
    "        right_iris = extract_iris(gray, [right_eye])\n",
    "\n",
    "        # Compute the gaze point\n",
    "        gaze_x, gaze_y = track_gaze(left_iris, right_iris)\n",
    "\n",
    "        # Draw a circle at the gaze point\n",
    "        cv2.circle(frame, (int(gaze_x), int(gaze_y)), 5, (0, 255, 0), -1)\n",
    "\n",
    "        # Draw lines from the pupils to the gaze point\n",
    "        cv2.line(frame, tuple(left_eye), (int(gaze_x), int(gaze_y)), (255, 0, 0), 2)\n",
    "        cv2.line(frame, tuple(right_eye), (int(gaze_x), int(gaze_y)), (255, 0, 0), 2)\n",
    "\n",
    "    # Display the image\n",
    "    cv2.imshow(\"Eye Tracker\", frame)\n",
    "\n",
    "    # Check for quit key\n",
    "    if cv2.waitKey(1) == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Release the video capture stream\n",
    "screen.release()\n",
    "\n",
    "# Close all windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
